Â¡Por supuesto, Adrian! AquÃ­ tienes todo el contenido completo del `README.md` listo para **copiar y pegar directamente** dentro de tu archivo `/codigo/README.md`:

---

```markdown
# Tarea 3 â€“ Procesamiento de Datos con Apache Spark

Este repositorio contiene el desarrollo de la **Tarea 3** del curso **Procesamiento de Datos con Apache Spark**, en la que se implementa una soluciÃ³n completa de anÃ¡lisis de datos en **modo batch y streaming** utilizando **Apache Spark** y **Apache Kafka**.

---

## ğŸ“ Estructura del repositorio

```
/codigo/
  â”œâ”€â”€ tarea3.py                   # Script para procesamiento batch de datos histÃ³ricos
  â”œâ”€â”€ spark_streaming_consumer.py # Script de consumo en tiempo real desde Kafka usando Spark Structured Streaming
  â””â”€â”€ README.md                   # Documento actual
```

---

## ğŸ§  Objetivo general

Implementar una soluciÃ³n distribuida de procesamiento de datos que permita analizar informaciÃ³n en batch y en tiempo real, integrando Apache Spark con Apache Kafka.

---

## ğŸš€ TecnologÃ­as utilizadas

- Apache Spark 3.4.4
- Apache Kafka 3.7.2
- Python 3.10
- Spark Structured Streaming
- kafka-python
- VirtualizaciÃ³n: UTM en macOS (Ubuntu 22.04)
- Herramientas: Spark Web UI, CLI, SSH, SCP

---

## ğŸ“„ DescripciÃ³n de scripts

### `tarea3.py`  
Procesamiento batch de un archivo CSV con datos de la Tasa de Cambio Representativa del Mercado (TRM).

**Operaciones realizadas:**
- Lectura del archivo `.csv` con `SparkSession.read.csv`
- Limpieza de columnas y tipos de datos
- AnÃ¡lisis exploratorio: `describe()`, `select()`, `orderBy()`
- CÃ¡lculos estadÃ­sticos y visualizaciÃ³n por consola

---

### `spark_streaming_consumer.py`  
Procesamiento de datos en tiempo real desde Kafka.

**Operaciones realizadas:**
- Lectura del stream de datos desde el topic `sensor_data`
- DefiniciÃ³n de esquema JSON para los mensajes
- ConversiÃ³n de timestamp y creaciÃ³n de ventanas de agregaciÃ³n
- CÃ¡lculo de promedios por sensor y ventana
- ImpresiÃ³n continua de resultados usando `writeStream`

---

## ğŸ§ª CÃ³mo ejecutar

### ğŸ”¹ 1. Procesamiento batch

```bash
spark-submit tarea3.py
```

### ğŸ”¹ 2. Streaming desde Kafka

1. Iniciar servicios de Kafka:

```bash
# En terminal 1
$KAFKA_HOME/bin/zookeeper-server-start.sh config/zookeeper.properties &

# En terminal 2
$KAFKA_HOME/bin/kafka-server-start.sh config/server.properties &
```

2. Ejecutar el productor simulado (opcional):
```bash
python3 kafka_producer.py
```

3. Ejecutar el consumidor Spark Streaming:

```bash
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 \
  spark_streaming_consumer.py
```

4. Abrir la consola de monitoreo:  
ğŸ‘‰ http://localhost:4040 (o tu IP local)

---

## ğŸ“¸ Evidencias del proceso

- Jobs, stages y DAGs visualizados desde Spark UI
- Batches y micro-lotes procesados en tiempo real
- EstadÃ­sticas de rendimiento desde â€œStructured Streamingâ€
- Consola de resultados por ventana y sensor_id

*(Las capturas estÃ¡n documentadas en el informe y presentaciÃ³n adjunta.)*

---

## ğŸ“š Referencias

- Apache Spark documentation: https://spark.apache.org/docs/latest/
- Kafka documentation: https://kafka.apache.org/documentation/
- UNAD â€“ Contenido del curso virtual
- Owen, S. et al. (2017). *Advanced Analytics with Spark: Patterns for Learning from Data at Scale*. Oâ€™Reilly Media.

---

## ğŸ‘¨â€ğŸ’» Autor

**Adrian Ramirez**  
Estudiante de IngenierÃ­a de Sistemas â€“ UNAD  
Abril 2025

---

